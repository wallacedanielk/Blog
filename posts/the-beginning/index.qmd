---
title: "Using Scikit Learn and SMOTE To Identify Employee Attrition"
author: "Daniel Wallace"
date: "2023-04-10"
categories: [machine Learning, Python]
image: "cogs.jpg"
---
<div>
<h3> Why are we here? </h3>
<p>
&emsp; Every employer, at one point or another, faces employee turnover. Given the cost of recruiting a new employee,
on-boarding, training, and the learning curve that a new employee faces to be able to bring value to their new position,
being able to prevent the loss of a seasoned employee holds value to the employer.
</p>
<p>
&emsp; We are going to look at a publicily availble dataset that is popular on Kaggle (<a href=https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset> Available Here </a>) and walk through preprocessing, feature selection, oversampling, and finally classification of the label class. 
</p>
<h3>It is all about the data </h3>
<p>

There is a little bit of prep that needs to be done before we dig into the data and code. If you would like to follow along, here is a link to the python environment file that I used in this research: <a href="https://github.com/wallacedanielk/fileHosting/blob/main/environment.yml">Here is my environment File.</a>

&emsp; The IBM HR Attrition data set is a fictional dataset created by IBM scientists. This dataset has been used in multiple papers and in various
notebooks on the Kaggle competition website. The attrition dataset has 1470 rows, with 35 columns . The label in this
case is yes or no, where yes implies that the employee has left the organization and no implies that they stayed. The
’yes’ result is 84% of the data and the ’no’ result is 16%.
</p>


![First we are going to import all required libraries for this analysis](importImage.png){width=80%}

To load the dataset you can read it in using Pandas, I call my data variable attritionData.  

![Before I get started I split my data into a dataset of features and an array of labels.](dataSplit.png){width=80%}


<h3>Time for a little housekeeping</h3>


![Labels are encoded using 1 for 'yes' and 0 for 'no'](encode.png){width=80%} 

![Columns 'Over18', 'EmployeeCount', 'Standard Hours' were dropped due to having the same value for all of the records in the dataset.The Column 'EmployeeNumber' was dropped due to it being a unique identifier for each record and offers no value to the model.](removeColumns.png){width=30%} 

![This dataset did not have any missing value, so no techniques for missing data were used. The above code will verify that.](missing.png){width=80%} 
 
![ All categorical variables were one-hot encoded using the 'dummies' Pandas package in Python.](oneHot.png){width=80%} 
 
 ![The continous features were scaled using standard scalar from Scikit Learn package in Python. I implement this in a method that we will call on the data.](scaleMethod.png){width=80%}

 ![Then you call the method that we created on the data, and get the columns for later use. ](scaleMethodUsed.png){width=80%}

 ![The dataset was split into training as testing data using a 70/30 split. ](dataSplitTestTrain.png){width=80%}






The testing data was not resampled to provide a clean test dataset to test the algorithms against. 


<h3>Selecting the right features</h3>

 ![Columns were dropped based on feature importances with a threshold of 0.015 ](importances.png){width=80%}

![The above section of code keeps only the 'important' columns](ColToKeep.png){width=80%}

<h3>Creating new data from old data</h3>

 ![The training data was resampled using borderline SMOTE. ](resample.png){width=80%}



<h3>Classifying the data</h3>
 

 ![We are using various machine learning models,  here we intialize the classifier and fit the models. ](classifierFit.png){width=80%}

 ![Now we make predictions](predictions.png){width=80%}

<h3>How do we know if the predictions are Good?</h3>
 
 ![We use precision, recall, f1-score, and accuracy](metrics.png){width=80%}

  ![The above metrics are calculated using a contingency table. Here is an example of one using our Logistic Regression Predictions](twoByTwo.png){width=80%}

  ![Another metric that you can use is the Area under the ROC Curve](ROC.png){width=80%}

![The code above produces a grphic like this one](ROCCurve.png){width=80%}

 
<h3>The Wrap Up</h3>

We walked through the analysis from data import all the way through testing the classification results. You should be able to conduct your own analysis or extend mine to meet your needs. After going through all of the above, on all of my classifiers I achieved the following results: 

To determine the best classification model to use to predict employee attrition we compare the metrics that were defined
above. Of the Non-Smote samples, Random Forest has a better average precision than all of the other models. XGBoost
has the best average recall. Both XGBoost and Random forest in the Non-SMOTE re-sampling category have accuracy
of 0.86. Depending on which level of errors you can live with in your research will allow you to choose either Random
Forest or XBBoost for your attrition data.
When comparing re-sampling vs not re-sampling we are again divided. Random Forest and XGBoost both had better
average precision, however the SMOTE re-sampling datasets yielded a better average recall. The accuracy was the same
for both re-sampled and non re-sampled. Again, if your research is looking to maximize average recall, then SMOTE
re-sampling with Random Forest is the best classification algorithm to use.
This dataset was relatively small, you also have to consider the computational complexity of SMOTE re-sampling to
determine if the trade off is worth any gains that you may make in average recall. In comparing most of the classification
algorithms in this research, SMOTE resampling underperformed the classification done with non-SMOTE re-sampled
data. Looking at the ROC Curve in figure 3 we can also see the advantage in using the non-SMOTE sampled dataset
for classification.
</div>






