---
title: "Feature Selection"
author: "Daniel Wallace"
date: "2023-02-26"
categories: [github]
image: "features.jpg"
---

Now we get into the weeds a little bit with some feature selection.  I am selecting features in two ways. First, I want to kick out columns that just do not make sense. For example, there is a column titled "over18", however it is true for EVERY record in the data set. This does not bring any useful information. So, based on that I manually removed three columns (EmployeeCount, Over18, StandardHours) as these had the exact same value for each row in the data set. I also removed the unique identifier, as this is unique for each row, again not providing any value for expanding this to future data. 

Next, we have a little fun.  I check for nulls in columns, one hot encode some columns, and then scale all numeric columns.  There were no nulls in this data set, so that saves me some time having to deal with them, and you some time for having to read about how I dealt with them. One hot oncoding was done using pandas 'dummies' funtion. I find this to be the easiest since it is one line of code to hande the entire dataset.  then I used Sci Kit Learn's built in standard scaler to scale my columns since the magnitude of values was very different between columns. For example, age vs monthly income. 

Lastly, I used a random forest with some of Sci Kit Learn's built in functionality to determine which columns were worth keeping. I used a threshold of 0.01 and then got rid of all of the other columns. I may regret that threshold, we will see later in the classification if 0.01 was a good choice or not. Then I split the data into testing and training, stratified by category at a 30% sampling. 

That was a lot, and I spend a good bit of time working through it. However, this is the longest part of the work, the classification will now be quick and painless. 