[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is about a machine learning project I worked on during my Masters degree at UWF."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Project",
    "section": "",
    "text": "Using Scikit Learn and SMOTE To Identify Employee Attrition\n\n\n\n\n\n\n\nmachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nDaniel Wallace\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/the-beginning/index.html",
    "href": "posts/the-beginning/index.html",
    "title": "Using Scikit Learn and SMOTE To Identify Employee Attrition",
    "section": "",
    "text": "Why are we here?\n\n\n  Every employer, at one point or another, faces employee turnover. Given the cost of recruiting a new employee, on-boarding, training, and the learning curve that a new employee faces to be able to bring value to their new position, being able to prevent the loss of a seasoned employee holds value to the employer.\n\n\n  We are going to look at a publicily availble dataset that is popular on Kaggle ( Available Here ) and walk through preprocessing, feature selection, oversampling, and finally classification of the label class.\n\n\nIt is all about the data\n\n\nThere is a little bit of prep that needs to be done before we dig into the data and code. If you would like to follow along, here is a link to the python environment file that I used in this research: Here is my environment File.\n  The IBM HR Attrition data set is a fictional dataset created by IBM scientists. This dataset has been used in multiple papers and in various notebooks on the Kaggle competition website. The attrition dataset has 1470 rows, with 35 columns . The label in this case is yes or no, where yes implies that the employee has left the organization and no implies that they stayed. The ’yes’ result is 84% of the data and the ’no’ result is 16%.\n\n\n\n\nFirst we are going to import all required libraries for this analysis\n\n\nTo load the dataset you can read it in using Pandas, I call my data variable attritionData.\n\n\n\nBefore I get started I split my data into a dataset of features and an array of labels.\n\n\n\nTime for a little housekeeping\n\n\n\n\nLabels are encoded using 1 for ‘yes’ and 0 for ‘no’\n\n\n\n\n\nColumns ‘Over18’, ‘EmployeeCount’, ‘Standard Hours’ were dropped due to having the same value for all of the records in the dataset.The Column ‘EmployeeNumber’ was dropped due to it being a unique identifier for each record and offers no value to the model.\n\n\n\n\n\nThis dataset did not have any missing value, so no techniques for missing data were used. The above code will verify that.\n\n\n\n\n\nAll categorical variables were one-hot encoded using the ‘dummies’ Pandas package in Python.\n\n\n\n\n\nThe continous features were scaled using standard scalar from Scikit Learn package in Python. I implement this in a method that we will call on the data.\n\n\n\n\n\nThen you call the method that we created on the data, and get the columns for later use.\n\n\n\n\n\nThe dataset was split into training as testing data using a 70/30 split.\n\n\nThe testing data was not resampled to provide a clean test dataset to test the algorithms against.\n\nSelecting the right features\n\n\n\n\nColumns were dropped based on feature importances with a threshold of 0.015\n\n\n\n\n\nThe above section of code keeps only the ‘important’ columns\n\n\n\nCreating new data from old data\n\n\n\n\nThe training data was resampled using borderline SMOTE.\n\n\n\nClassifying the data\n\n\n\n\nWe are using various machine learning models, here we intialize the classifier and fit the models.\n\n\n\n\n\nNow we make predictions\n\n\n\nThe Wrap Up"
  },
  {
    "objectID": "posts/the-first-dataset/index.html",
    "href": "posts/the-first-dataset/index.html",
    "title": "Its All About the Data",
    "section": "",
    "text": "In the beginning there was data. Every good machine learning project starts with a question. Every question is answered by the data. So my goal is to ask interesting questions and hope that my data can answer them.\nMy first question is a simple one: I employ a diverse group of talented people, how do I know when I am about to lose one of them? Well, you could start with building relationships and just asking. However, let’s say that I employ a lot of people, and my middle managers are not exactly keeping up with the temperature at the office, so let’s see what I can answer with data.\nWhat is my data set you ask? Here is a link:\nhttps://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset\nI am starting with 34 features and one label (‘attrition’) that simply says yes they left, or no they stayed.\nThe dataaset has 1470 rows of data. The data is synthetic data generated by IBM, so some of the usual problems do not exist (missing data, accidental entries out of range) so, in that case my data is relatively clean. I will need to do some scaling, change some categories to numerics, and select the features that I will use. I also have some imbalance in the data that I will need to deal with.\nI am sure I am not the first person to think about this and not the first person to use this data, so I will first dig into what other people have done and if I like it or if I can find a novel approach.\nNext time, we start preprocessing!"
  },
  {
    "objectID": "posts/first-set-of-results/index.html",
    "href": "posts/first-set-of-results/index.html",
    "title": "Classification and Results",
    "section": "",
    "text": "The data is cleaned, the features are selected, now what? Well, I think it is time to get some results! For this first set of results, we are doing something simple. We are trying to predict employee attrition. This first attempt just compares five models to see which models give the best results so that we can move on to more questions. I am comparing the following classifiers:\n\n\n\nLogistic Regression\n\n\nSupport Vector Machines\n\n\nDecision Trees\n\n\nK-Nearest Neighbor\n\n\nRandom Forest\n\n\n\nTo train these models, I am using Python’s Scikitlearn package. I am using 4 metrics to compare the results:\n\n\n\nAcuracy\n\n\nPrecision\n\n\nRecall\n\n\nf_score\n\n\n\nAnd the Results are in:\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\nSVM\n\n\nDecision Tree\n\n\nKNN\n\n\nRandom Forest\n\n\n\n\n\n\naccuracy \n\n\n0.828\n\n\n0.841 \n\n\n0.839 \n\n\n0.844 \n\n\n0.855 \n\n\n\n\nprecision\n\n\n0.470 \n\n\n0.511 \n\n\n0.5 \n\n\n0.556 \n\n\n0.706 \n\n\n\n\nrecall\n\n\n 0.550\n\n\n0.324 \n\n\n0.096 \n\n\n0.141 \n\n\n0.170 \n\n\n\n\nf_score \n\n\n0.510 \n\n\n0.397 \n\n\n0.165 \n\n\n0.225 \n\n\n0.273"
  },
  {
    "objectID": "posts/feature_selection/index.html",
    "href": "posts/feature_selection/index.html",
    "title": "Feature Selection",
    "section": "",
    "text": "Now we get into the weeds a little bit with some feature selection. I am selecting features in two ways. First, I want to kick out columns that just do not make sense. For example, there is a column titled “over18”, however it is true for EVERY record in the data set. This does not bring any useful information. So, based on that I manually removed three columns (EmployeeCount, Over18, StandardHours) as these had the exact same value for each row in the data set. I also removed the unique identifier, as this is unique for each row, again not providing any value for expanding this to future data.\nNext, we have a little fun. I check for nulls in columns, one hot encode some columns, and then scale all numeric columns. There were no nulls in this data set, so that saves me some time having to deal with them, and you some time for having to read about how I dealt with them. One hot oncoding was done using pandas ‘dummies’ funtion. I find this to be the easiest since it is one line of code to hande the entire dataset. then I used Sci Kit Learn’s built in standard scaler to scale my columns since the magnitude of values was very different between columns. For example, age vs monthly income.\nLastly, I used a random forest with some of Sci Kit Learn’s built in functionality to determine which columns were worth keeping. I used a threshold of 0.01 and then got rid of all of the other columns. I may regret that threshold, we will see later in the classification if 0.01 was a good choice or not. Then I split the data into testing and training, stratified by category at a 30% sampling.\nThat was a lot, and I spend a good bit of time working through it. However, this is the longest part of the work, the classification will now be quick and painless."
  },
  {
    "objectID": "posts/i-think-it-worked/index.html",
    "href": "posts/i-think-it-worked/index.html",
    "title": "Publish to GitHub and Beyond",
    "section": "",
    "text": "Well, if you are reading this post I did it!!\nI created a Quarto Blog, added Blog pages, and published this to GitHub, and added some CI/CD using Github actions to automate my build and deployment process. Now it is on to the next part of the project. Lets go clean some data!"
  }
]